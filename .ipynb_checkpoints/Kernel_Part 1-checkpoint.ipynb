{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心课程 \n",
    "## Part 1 Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides an overview of the program and introduces the fundamentals of Natural Language Processing through symbolic manipulation, including text cleaning, normalization, and tokenization. You'll then build a part of speech tagger using hidden Markov models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目：<a> Part of Speech Tagging</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "- Welcome to Natural Language Processing\n",
    "- Udacity Support \n",
    "- Intro to NLP\n",
    "- Text Processing\n",
    "- Spam Classifier with Naive Bayes\n",
    "- Part of Speech Tagging with HMMs\n",
    "- Project: Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Intro to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 结构化语言(Structured Languages)：为什么计算机理解我们很难呢？ \n",
    "人类语言的一大缺陷，依赖于你观察方法的特征，是缺乏准确定义的结构。 \n",
    "\n",
    "为了理解困难的原因，我们首先观察更具**结构化的语言**。 \n",
    "\n",
    "例如：    \n",
    "**数学使用一种结构化语言**，当我写出y=2x+5时，我想表达的意思是非常清晰的。  \n",
    "我的意思是变量y与变量x的相关性是 2x+5，**形式逻辑也使用结构语言**。 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如：  \n",
    "考虑表达式父系(x,y)和父系(x,z)得到同层(y,z)，这个语句表明如果x是y的父系且x是z的父系，那么y和z是同层。\n",
    "\n",
    "<img src='./images/part1_001.png' width='70%'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你熟悉的结构化语言，是**脚本语言和编程语言**。\n",
    "\n",
    "思考这个SQL语句：\n",
    "> SELECT name, email FROM users WHERE name LIKE A%\n",
    "\n",
    "我们询问数据库 姓名以\"A\"开头，所有用户的姓名和电子邮箱地址。\n",
    "\n",
    "这些语言的设计，要尽可能清晰，适合计算机处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 语法Grammar\n",
    "对计算机而言，结构化语言容易分析和理解。   \n",
    "\n",
    "因为它们受到一套严格规则或语法的定义，这是表达语法和算法的标准形式。\n",
    "\n",
    "可以恰当地分析形成后的语句，理解它的确切含义。\n",
    " \n",
    "如果一个语句与既定语法不匹配，通常计算机无法猜测意思，它就会放弃。\n",
    "这样违反语法规则会报告为语法错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='./images/part1_002.png' width='50%'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 非结构化文本Unstructured Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用来交流的语言也界定了语法规则，实际上在一些情况下 我们使用**简单的结构化语句**。  \n",
    "\n",
    "不过大部分人类交谈很复杂，**非结构化**。\n",
    "\n",
    "尽管如此，我们似乎非常善于了解彼此，甚至这种模棱两可在某种程度上受到欢迎。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**那么计算机如何理解非结构化文本呢？**\n",
    "\n",
    "这是一些基本的想法。\n",
    "计算机可以在一定程度上处理词语和短语，试图找出\n",
    "- **关键词**\n",
    "- **词性**\n",
    "- **命名实体**\n",
    "- **日期和数量**等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='./images/part1_003.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这些信息他们也可以分析语句，至少有一项相对比较结构化，这可以帮助提取语句,问题和指令的相关部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从更高级来说，计算机可以\n",
    "- 分析文件\n",
    "- 找出常见词和罕见词\n",
    "- 评估所表达的整体语气或情感\n",
    "- 甚至可以对相似文件进行分类或分组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/part1_004.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以想象根据上面这些观点，计算机可以对非结构文本进行进行各种事情，即使它们并不像我们一样理解文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 词汇计数Counting Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算词汇频次Counting Word Frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As I was waiting, a man came out of a side room, and at a glance I was sure he must be Long John. His left leg was cut off close by the hip, and under the left shoulder he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird. He was very tall and strong, with a face as big as a ham—plain and pale, but intelligent and smiling. Indeed, he seemed in the most cheerful spirits, whistling as he moved about among the tables, with a merry word or a slap on the shoulder for the more favoured of his guests.\n",
    "\n",
    "— Excerpt from Treasure Island, by Robert Louis Stevenson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following coding exercise, we have provided code to load the text from a file, call the function count_words() to obtain word counts (which you need to implement), and print the 10 most common and least common unique words.\n",
    "\n",
    "Complete the portions marked as TODO to count how many times each unique word occurs in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Count words.'''\n",
    "import re\n",
    "\n",
    "def count_words(text):\n",
    "    '''Counting how many times each unique word occurs in text.'''\n",
    "    counts = dict() # dictionary of {<word> : <count>} pairs to return\n",
    "    \n",
    "    # TODO: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # TODO: Split text into tokens (words), leaving out punctuation \n",
    "    # (Hint: Use regex to split on non-alphanumeric characters)\n",
    "    regex = re.compile(r'\\b\\w+\\b')\n",
    "    words = regex.findall(text)\n",
    "    for x in set(words):\n",
    "        counts[x] = words.count(x)\n",
    "    \n",
    "    # TODO: Aggregate word counts using a dictionary\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run():\n",
    "    with open('./files/input.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "        counts = count_words(text)\n",
    "        sorted_counts = sorted(counts.items(), key=lambda pair: pair[1], reverse=True)\n",
    "        \n",
    "        print('10 most common words: \\nWord\\t Count')\n",
    "        for word, count in sorted_counts[:10]:\n",
    "            print('{} \\t {}'.format(word, count))\n",
    "            \n",
    "        print('\\n 10 least common words: \\nWord\\t Count')\n",
    "        for word, count in sorted_counts[-10:]:\n",
    "            print('{} \\t {}'.format(word, count))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common words: \n",
      "Word\t Count\n",
      "a \t 9\n",
      "the \t 6\n",
      "he \t 6\n",
      "and \t 5\n",
      "as \t 4\n",
      "was \t 4\n",
      "with \t 3\n",
      "i \t 2\n",
      "his \t 2\n",
      "shoulder \t 2\n",
      "\n",
      " 10 least common words: \n",
      "Word\t Count\n",
      "managed \t 1\n",
      "be \t 1\n",
      "waiting \t 1\n",
      "ham \t 1\n",
      "by \t 1\n",
      "tables \t 1\n",
      "side \t 1\n",
      "whistling \t 1\n",
      "big \t 1\n",
      "tall \t 1\n"
     ]
    }
   ],
   "source": [
    "test_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意：字典按value值进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {'a': 9, 'cut': 1, 'carried': 1, 'hip': 1, 'it': 1, 'merry': 1, 'man': 1, 'at': 1, 'pale': 1, 'bird': 1, 'which': 1, 'room': 1, 'indeed': 1, 'in': 1, 'must': 1, 'for': 1, 'among': 1, 'intelligent': 1, 'spirits': 1, 'face': 1, 'the': 6, 'guests': 1, 'i': 2, 'smiling': 1, 'leg': 1, 'close': 1, 'most': 1, 'his': 2, 'off': 1, 'he': 6, 'dexterity': 1, 'with': 3, 'but': 1, 'on': 1, 'cheerful': 1, 'shoulder': 2, 'word': 1, 'and': 5, 'came': 1, 'crutch': 1, 'favoured': 1, 'out': 1, 'about': 2, 'glance': 1, 'like': 1, 'seemed': 1, 'slap': 1, 'very': 1, 'under': 1, 'moved': 1, 'as': 4, 'upon': 1, 'strong': 1, 'of': 2, 'or': 1, 'plain': 1, 'left': 2, 'more': 1, 'wonderful': 1, 'john': 1, 'was': 4, 'long': 1, 'hopping': 1, 'sure': 1, 'managed': 1, 'be': 1, 'waiting': 1, 'ham': 1, 'by': 1, 'tables': 1, 'side': 1, 'whistling': 1, 'big': 1, 'tall': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 9), ('the', 6), ('he', 6), ('and', 5), ('as', 4)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_counts = sorted(counts.items(), key=lambda pair:pair[1], reverse=True)\n",
    "sorted_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "00:00:00,000 --> 00:00:04,019\n",
    "那么什么原因使计算机无法\n",
    "\n",
    "2\n",
    "00:00:04,019 --> 00:00:07,439\n",
    "像人类一样理解自然语言呢？\n",
    "\n",
    "3\n",
    "00:00:07,440 --> 00:00:12,300\n",
    "部分原因在于我们语句的差异性和复杂性\n",
    "\n",
    "4\n",
    "00:00:12,300 --> 00:00:15,210\n",
    "思考这个电影评论的片段\n",
    "\n",
    "5\n",
    "00:00:15,210 --> 00:00:18,000\n",
    "“我受到诱惑看到这些\n",
    "\n",
    "6\n",
    "00:00:18,000 --> 00:00:23,054\n",
    "过时陈旧的乐趣和诡计 我被骗了”\n",
    "\n",
    "7\n",
    "00:00:23,053 --> 00:00:24,538\n",
    "虽然最开始是\n",
    "\n",
    "8\n",
    "00:00:24,539 --> 00:00:30,059\n",
    "一些可能正面的词语 结果是非常负面的评论\n",
    "\n",
    "9\n",
    "00:00:30,059 --> 00:00:32,908\n",
    "对我们来说 这样的语句在某种程度上很有趣\n",
    "\n",
    "10\n",
    "00:00:32,908 --> 00:00:37,839\n",
    "但是电脑在分析它们时 可能会出错\n",
    "\n",
    "11\n",
    "00:00:37,840 --> 00:00:42,670\n",
    "不过让自然语言处理的难度超出你想象 存在越大的挑战\n",
    "\n",
    "12\n",
    "00:00:42,670 --> 00:00:45,054\n",
    "观察一下这个语句\n",
    "\n",
    "13\n",
    "00:00:45,054 --> 00:00:49,935\n",
    "“沙发没法通过门 因为它太窄了 (The sofa didn’t fit through the door because it was too narrow.)”\n",
    "\n",
    "14\n",
    "00:00:49,935 --> 00:00:52,164\n",
    "“它” 指代什么？\n",
    "\n",
    "15\n",
    "00:00:52,164 --> 00:00:55,515\n",
    "显然 “它” 指代门\n",
    "\n",
    "16\n",
    "00:00:55,515 --> 00:00:58,975\n",
    "现在思考下这句话的变体\n",
    "\n",
    "17\n",
    "00:00:58,975 --> 00:01:03,109\n",
    "“沙发没法通过门 因为它太宽了 (The sofa didn’t fit through the door because it was too wide.)”\n",
    "\n",
    "18\n",
    "00:01:03,109 --> 00:01:05,819\n",
    "这个例子中 “它” 指代什么？\n",
    "\n",
    "19\n",
    "00:01:05,819 --> 00:01:08,950\n",
    "这里指沙发 思考一下\n",
    "\n",
    "20\n",
    "00:01:08,950 --> 00:01:11,859\n",
    "为了理解这句话的恰当意思或语义\n",
    "\n",
    "21\n",
    "00:01:11,858 --> 00:01:16,583\n",
    "你暗中运用了对物质世界的理解\n",
    "\n",
    "22\n",
    "00:01:16,584 --> 00:01:20,230\n",
    "宽的东西无法通过窄的东西\n",
    "\n",
    "23\n",
    "00:01:20,230 --> 00:01:23,635\n",
    "你可能之前经历过类似的情况\n",
    "\n",
    "24\n",
    "00:01:23,635 --> 00:01:28,689\n",
    "可以想象一下数不胜数的其他场景\n",
    "\n",
    "25\n",
    "00:01:28,688 --> 00:01:34,000\n",
    "其中一些知识或语境是正确理解表达所必不可少的\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Context Is Everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "00:00:00,000 --> 00:00:03,825\n",
    "即使面临目前的挑战 自然语言处理\n",
    "\n",
    "2\n",
    "00:00:03,825 --> 00:00:05,490\n",
    "也充满前景\n",
    "\n",
    "3\n",
    "00:00:05,490 --> 00:00:09,351\n",
    "为了帮助我们更好理解今天如何运用自然语言处理\n",
    "\n",
    "4\n",
    "00:00:09,351 --> 00:00:13,285\n",
    "我们邀请了 IBM 公司的 Armen Pischdotchian\n",
    "\n",
    "5\n",
    "00:00:13,285 --> 00:00:15,314\n",
    "你好 我是 Armen\n",
    "\n",
    "6\n",
    "00:00:15,314 --> 00:00:19,230\n",
    "目前担任学术科技导师 我使用 Watson 认知服务\n",
    "\n",
    "7\n",
    "00:00:19,230 --> 00:00:23,234\n",
    "为大学 高中\n",
    "\n",
    "8\n",
    "00:00:23,234 --> 00:00:27,129\n",
    "STEM 专业和女程序员进行讲课 组织工作坊\n",
    "\n",
    "9\n",
    "00:00:27,129 --> 00:00:32,689\n",
    "我们也使用 IBM Watson 认知服务运行编程马拉松 (hackathon)\n",
    "\n",
    "10\n",
    "00:00:32,689 --> 00:00:34,914\n",
    "这项服务位于 Bluemix 服务平台上\n",
    "\n",
    "11\n",
    "00:00:34,914 --> 00:00:40,588\n",
    "我们做的另一项工作是经常装扮成系统架构师\n",
    "\n",
    "12\n",
    "00:00:40,588 --> 00:00:43,798\n",
    "因为大学也与客户进行合作 这些客户\n",
    "\n",
    "13\n",
    "00:00:43,798 --> 00:00:47,689\n",
    "使用学术环境做为开发场地\n",
    "\n",
    "14\n",
    "00:00:47,689 --> 00:00:51,719\n",
    "我非常期待这类拓展式合作 我会\n",
    "\n",
    "15\n",
    "00:00:51,719 --> 00:00:56,835\n",
    "竭力为这样的客户构建概念实证 你说的没错 Arpan\n",
    "\n",
    "16\n",
    "00:00:56,835 --> 00:01:02,490\n",
    "我们已经了解到 越来越多的项目使用不同方式的自然语言处理\n",
    "\n",
    "17\n",
    "00:01:02,490 --> 00:01:05,819\n",
    "如今自然语言处理 并不是单独使用\n",
    "\n",
    "18\n",
    "00:01:05,819 --> 00:01:11,189\n",
    "而是嵌入在智能端对端解决方案的一部分 使用大量各种技术\n",
    "\n",
    "19\n",
    "00:01:11,188 --> 00:01:14,143\n",
    "通常提升认知视野 \n",
    "\n",
    "20\n",
    "00:01:14,144 --> 00:01:17,605\n",
    "而不只是纸上谈兵\n",
    "\n",
    "21\n",
    "00:01:17,605 --> 00:01:23,800\n",
    "我明白了 那你认为自然语言处理在这样系统中的主要角色是什么呢？\n",
    "\n",
    "22\n",
    "00:01:23,799 --> 00:01:29,128\n",
    "自然语言处理在计算机科学中是一大难题\n",
    "\n",
    "23\n",
    "00:01:29,129 --> 00:01:33,015\n",
    "人类语言几乎不准确 无法清晰表达\n",
    "\n",
    "24\n",
    "00:01:33,015 --> 00:01:37,064\n",
    "理解人类语言 不仅是要理解词汇\n",
    "\n",
    "25\n",
    "00:01:37,063 --> 00:01:41,468\n",
    "还要理解概念 以及如何组合在一起形成意思\n",
    "\n",
    "26\n",
    "00:01:41,468 --> 00:01:45,411\n",
    "虽然对人类来说语言是很容易学习的\n",
    "\n",
    "27\n",
    "00:01:45,412 --> 00:01:47,819\n",
    "但是语言的模糊性导致\n",
    "\n",
    "28\n",
    "00:01:47,819 --> 00:01:53,069\n",
    "计算机难以掌握自然语言处理\n",
    "\n",
    "29\n",
    "00:01:53,069 --> 00:01:55,191\n",
    "例如思考这个短语\n",
    "\n",
    "30\n",
    "00:01:55,191 --> 00:01:58,650\n",
    "这个喷泉不是饮用水 (this fountain is not drinking water)\n",
    "\n",
    "31\n",
    "00:01:58,650 --> 00:02:01,569\n",
    "认知系统可以对词组拆分字段 分析\n",
    "\n",
    "32\n",
    "00:02:01,569 --> 00:02:03,859\n",
    "并添加注释 推断出\n",
    "\n",
    "33\n",
    "00:02:03,858 --> 00:02:08,210\n",
    "喷泉与喝水行为无关\n",
    "\n",
    "34\n",
    "00:02:08,210 --> 00:02:13,555\n",
    "不过我们知道 这是说我们不应该从这个喷泉中喝水\n",
    "\n",
    "35\n",
    "00:02:13,555 --> 00:02:17,259\n",
    "如今的系统不仅可以分析话语\n",
    "\n",
    "36\n",
    "00:02:17,258 --> 00:02:19,848\n",
    "它们理解词语之间的联系\n",
    "\n",
    "37\n",
    "00:02:19,848 --> 00:02:23,718\n",
    "话语前后的语境\n",
    "\n",
    "38\n",
    "00:02:23,718 --> 00:02:28,859\n",
    "例如我可以更好地用语言描述自己的需求时\n",
    "\n",
    "39\n",
    "00:02:28,860 --> 00:02:34,755\n",
    "为什么需要浏览几十个 Airbnb 评论 找到适合的住宿呢？\n",
    "\n",
    "40\n",
    "00:02:34,754 --> 00:02:37,519\n",
    "我要寻找一个可以看到河流的公寓\n",
    "\n",
    "41\n",
    "00:02:37,520 --> 00:02:39,750\n",
    "可以俯瞰曼哈顿\n",
    "\n",
    "42\n",
    "00:02:39,750 --> 00:02:43,128\n",
    "但我不想要嘈杂和脏臭的位置\n",
    "\n",
    "43\n",
    "00:02:43,128 --> 00:02:45,948\n",
    "我希望认知系统可以找到\n",
    "\n",
    "44\n",
    "00:02:45,949 --> 00:02:50,895\n",
    "顾客的评论 准确描述我要找的标准\n",
    "\n",
    "45\n",
    "00:02:50,895 --> 00:02:53,604\n",
    "因此自然语言处理派上用场\n",
    "\n",
    "46\n",
    "00:02:53,604 --> 00:02:57,169\n",
    "它可以帮助理解用户的意图\n",
    "\n",
    "47\n",
    "00:02:57,169 --> 00:03:00,919\n",
    "了解他们的喜恶 处理大量文本\n",
    "\n",
    "48\n",
    "00:03:00,919 --> 00:03:03,530\n",
    "来更好地推断我们对话的语境\n",
    "\n",
    "49\n",
    "00:03:03,530 --> 00:03:09,033\n",
    "和我们所表达的准确含义\n",
    "\n",
    "50\n",
    "00:03:09,033 --> 00:03:11,174\n",
    "这是为你准备的例子 Arpan\n",
    "\n",
    "51\n",
    "00:03:11,175 --> 00:03:16,474\n",
    "聪明人与聪明的家伙肯定不相同 (A wise man is absolutely not the\n",
    "same as a wise guy)\n",
    "\n",
    "52\n",
    "00:03:16,473 --> 00:03:21,000\n",
    "思考这句话 机会渺茫相当于机会很小 (a slim chance is the same as a\n",
    "fat chance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 NLP and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "00:00:00,000 --> 00:00:04,458\n",
    "你刚才提到与大学和相关客户合作\n",
    "\n",
    "2\n",
    "00:00:04,458 --> 00:00:09,734\n",
    "构建认知解决方案 解决他们棘手的问题\n",
    "\n",
    "3\n",
    "00:00:09,734 --> 00:00:14,884\n",
    "你可以给我们介绍这些项目中自然语言处理发挥关键作用的示例吗？\n",
    "\n",
    "4\n",
    "00:00:14,884 --> 00:00:16,730\n",
    "非常乐意 Arpun\n",
    "\n",
    "5\n",
    "00:00:16,730 --> 00:00:21,897\n",
    "我印象深刻的一次合作是在纽约市的圣约翰大学\n",
    "\n",
    "6\n",
    "00:00:21,897 --> 00:00:24,600\n",
    "准确说应该是教师学院\n",
    "\n",
    "7\n",
    "00:00:24,600 --> 00:00:28,774\n",
    "听众包括教师 实践导师\n",
    "\n",
    "8\n",
    "00:00:28,774 --> 00:00:33,149\n",
    "人生导师 C 语言导师等\n",
    "\n",
    "9\n",
    "00:00:33,149 --> 00:00:35,658\n",
    "他们都是主题专家\n",
    "\n",
    "10\n",
    "00:00:35,658 --> 00:00:39,859\n",
    "他们寻找衡量如何高效的标准\n",
    "\n",
    "11\n",
    "00:00:39,859 --> 00:00:45,189\n",
    "让教导学徒导师成为专家 独立承担任务\n",
    "\n",
    "12\n",
    "00:00:45,189 --> 00:00:49,469\n",
    "我建议我们录制教师\n",
    "\n",
    "13\n",
    "00:00:49,469 --> 00:00:52,320\n",
    "与学徒之间的面试\n",
    "\n",
    "14\n",
    "00:00:52,320 --> 00:00:54,844\n",
    "让硕士生学习训练\n",
    "\n",
    "15\n",
    "00:00:54,844 --> 00:00:59,000\n",
    "语音到文本是这里第一个认知服务\n",
    "\n",
    "16\n",
    "00:00:59,000 --> 00:01:04,280\n",
    "这可以理解双方的话语\n",
    "\n",
    "17\n",
    "00:01:04,280 --> 00:01:09,319\n",
    "然后我们运行音调分析器和情感分析\n",
    "\n",
    "18\n",
    "00:01:09,319 --> 00:01:14,584\n",
    "通过高亮语句进一步揭露学生表现出来的开放性\n",
    "\n",
    "19\n",
    "00:01:14,584 --> 00:01:17,555\n",
    "责任感 外向性\n",
    "\n",
    "20\n",
    "00:01:17,555 --> 00:01:22,250\n",
    "亲和性和情绪不稳定性\n",
    "\n",
    "21\n",
    "00:01:22,250 --> 00:01:27,129\n",
    "你可以联机改变语句 观察颜色代码的变化\n",
    "\n",
    "22\n",
    "00:01:27,129 --> 00:01:31,924\n",
    "顺便说一下 心理学家在分析文本中使用五大人格特点\n",
    "\n",
    "23\n",
    "00:01:31,924 --> 00:01:38,064\n",
    "对每个话语返回置信度\n",
    "\n",
    "24\n",
    "00:01:38,063 --> 00:01:42,649\n",
    "这是另一个例子 Watson 肿瘤解决方案 (Watson for Oncology) 帮助外科医生\n",
    "\n",
    "25\n",
    "00:01:42,650 --> 00:01:47,355\n",
    "迅速确认患者病例中的关键信息\n",
    "\n",
    "26\n",
    "00:01:47,355 --> 00:01:50,150\n",
    "调查相关文章 并且探索\n",
    "\n",
    "27\n",
    "00:01:50,150 --> 00:01:54,364\n",
    "减少多余护理变化的治疗选择\n",
    "\n",
    "28\n",
    "00:01:54,364 --> 00:01:56,689\n",
    "然后把时间反馈给他们的病人\n",
    "\n",
    "29\n",
    "00:01:56,688 --> 00:02:01,142\n",
    "印度班加罗尔的马尼帕尔医院\n",
    "\n",
    "30\n",
    "00:02:01,143 --> 00:02:04,870\n",
    "对 Watson 进行试点 对比基于\n",
    "\n",
    "31\n",
    "00:02:04,870 --> 00:02:11,093\n",
    "Watson 肿瘤解决方案的建议和马尼帕尔医院肿瘤团队的建议\n",
    "\n",
    "32\n",
    "00:02:11,093 --> 00:02:14,620\n",
    "这是个 12 到 15 名癌症专家组成的团队\n",
    "\n",
    "33\n",
    "00:02:14,620 --> 00:02:18,289\n",
    "每周审查最复杂的癌症案例\n",
    "\n",
    "34\n",
    "00:02:18,288 --> 00:02:24,098\n",
    "在双盲法研究中 马尼帕尔的医生发现 Watson\n",
    "\n",
    "35\n",
    "00:02:24,098 --> 00:02:31,375\n",
    "和肿瘤团队对 90% 乳腺癌案例的建议是一致的\n",
    "\n",
    "36\n",
    "00:02:31,375 --> 00:02:35,240\n",
    "这项研究发表在圣安东尼奥乳腺癌研讨会上\n",
    "\n",
    "37\n",
    "00:02:35,240 --> 00:02:41,050\n",
    "哇塞 看到 Watson 如何用于解决这些巨大问题真不错\n",
    "\n",
    "38\n",
    "00:02:41,050 --> 00:02:43,900\n",
    "自然语言处理还有没有其他形式\n",
    "\n",
    "39\n",
    "00:02:43,900 --> 00:02:47,450\n",
    "融入到日常产品和服务中呢？\n",
    "\n",
    "40\n",
    "00:02:47,449 --> 00:02:50,228\n",
    "谢谢你的提问 Arpun\n",
    "\n",
    "41\n",
    "00:02:50,229 --> 00:02:54,145\n",
    "这样来思考 首先以前的网站\n",
    "\n",
    "42\n",
    "00:02:54,145 --> 00:02:58,280\n",
    "然后应用程序 现在的机器人程序\n",
    "\n",
    "43\n",
    "00:02:58,280 --> 00:03:01,764\n",
    "我们来思考形式上的三个转变\n",
    "\n",
    "44\n",
    "00:03:01,764 --> 00:03:06,585\n",
    "其中自然语言处理改变了我们对周围世界的认知和行为\n",
    "\n",
    "45\n",
    "00:03:06,585 --> 00:03:10,430\n",
    "首先是一对一和一对多\n",
    "\n",
    "46\n",
    "00:03:10,430 --> 00:03:14,150\n",
    "第一次出现在广告营销历史中\n",
    "\n",
    "47\n",
    "00:03:14,150 --> 00:03:17,509\n",
    "品牌有机会从个人角度接触到消费者\n",
    "\n",
    "48\n",
    "00:03:17,508 --> 00:03:23,688\n",
    "以消费者口述出来的速度进行传播\n",
    "\n",
    "49\n",
    "00:03:23,688 --> 00:03:26,810\n",
    "在对话式世界中营销人员不再\n",
    "\n",
    "50\n",
    "00:03:26,810 --> 00:03:31,085\n",
    "依靠叫卖获得最好的结果\n",
    "\n",
    "51\n",
    "00:03:31,085 --> 00:03:33,699\n",
    "他们需要聊天和聆听\n",
    "\n",
    "52\n",
    "00:03:33,699 --> 00:03:39,180\n",
    "第二是小数据和大数据\n",
    "\n",
    "53\n",
    "00:03:39,180 --> 00:03:44,620\n",
    "这种一对一的交流可以给你提供个人见解和特性\n",
    "\n",
    "54\n",
    "00:03:44,620 --> 00:03:49,944\n",
    "提供简单易懂可行的数据\n",
    "\n",
    "55\n",
    "00:03:49,943 --> 00:03:55,839\n",
    "它第一次以人类自然语境的方式 建立了信息排序的完美平台\n",
    "\n",
    "56\n",
    "00:03:55,840 --> 00:04:01,670\n",
    "第三 永远在线和永远完美\n",
    "\n",
    "57\n",
    "00:04:01,669 --> 00:04:03,899\n",
    "真实和原始的崛起\n",
    "\n",
    "58\n",
    "00:04:03,900 --> 00:04:08,210\n",
    "Periscope 和 Snapchat 等直播应用程序\n",
    "\n",
    "59\n",
    "00:04:08,210 --> 00:04:14,014\n",
    "代表了千禧一代和 Z 一代青少年依赖的最新社交货币\n",
    "\n",
    "60\n",
    "00:04:14,014 --> 00:04:18,588\n",
    "所以永远在线 保持真实\n",
    "\n",
    "61\n",
    "00:04:18,588 --> 00:04:23,295\n",
    "对一个品牌更重要 而不是完美的图像或像素\n",
    "\n",
    "62\n",
    "00:04:23,295 --> 00:04:27,588\n",
    "那样只会偶尔在线 永远迟到\n",
    "\n",
    "63\n",
    "00:04:27,588 --> 00:04:33,555\n",
    "每个对话都记录成日志 这样你可以保持追踪\n",
    "\n",
    "64\n",
    "00:04:33,555 --> 00:04:38,939\n",
    "重复 经过一段时间提升你的机器人程序质量\n",
    "\n",
    "65\n",
    "00:04:38,939 --> 00:04:45,712\n",
    "同样 你在手机或汽车上使用的语音界面\n",
    "\n",
    "66\n",
    "00:04:45,711 --> 00:04:50,365\n",
    "它们应用了自然语言处理 理解你所说的内容\n",
    "\n",
    "67\n",
    "00:04:50,365 --> 00:04:53,865\n",
    "这是因为语音是极具噪音来源的输入\n",
    "\n",
    "68\n",
    "00:04:53,865 --> 00:04:59,399\n",
    "如果没有智能 很难处理\n",
    "\n",
    "69\n",
    "00:04:59,399 --> 00:05:05,673\n",
    "去年 IBM 在对话式语音识别方面宣布了一项重大里程碑\n",
    "\n",
    "70\n",
    "00:05:05,673 --> 00:05:11,039\n",
    "这个系统实现了 6.9% 词语错误率\n",
    "\n",
    "71\n",
    "00:05:11,040 --> 00:05:16,319\n",
    "此后我们继续推动语音识别的边界\n",
    "\n",
    "72\n",
    "00:05:16,319 --> 00:05:23,850\n",
    "今天我们已经实现了 5.5% 词语错误率 打破业界记录\n",
    "\n",
    "73\n",
    "00:05:23,850 --> 00:05:28,274\n",
    "这是通过极其困难的语音识别任务衡量的\n",
    "\n",
    "74\n",
    "00:05:28,274 --> 00:05:35,100\n",
    "记录人们日常话题的讨论对话 例如买车\n",
    "\n",
    "75\n",
    "00:05:35,100 --> 00:05:40,209\n",
    "录音后的语料库就是 Switchboard\n",
    "\n",
    "76\n",
    "00:05:40,209 --> 00:05:45,954\n",
    "在过去二十年中这成为语音识别系统的标准检查程序\n",
    "\n",
    "77\n",
    "00:05:45,954 --> 00:05:47,740\n",
    "我们实现了突破\n",
    "\n",
    "78\n",
    "00:05:47,740 --> 00:05:50,555\n",
    "5.5% 成为巨大突破\n",
    "\n",
    "79\n",
    "00:05:50,555 --> 00:05:55,069\n",
    "相较于人类词语错误率是 5.1%\n",
    "\n",
    "80\n",
    "00:05:55,069 --> 00:05:58,160\n",
    "这表明我们找到了合适的方法\n",
    "\n",
    "81\n",
    "00:05:58,160 --> 00:06:03,000\n",
    "然后可以认为语音识别技术能够和人类媲美\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 How NLP Pipelines Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "00:00:00,000 --> 00:00:03,600\n",
    "我们现在详细介绍文本处理\n",
    "\n",
    "2\n",
    "00:00:03,600 --> 00:00:06,233\n",
    "我们想到的第一个问题是\n",
    "\n",
    "3\n",
    "00:00:06,232 --> 00:00:08,550\n",
    "为什么要处理文本？\n",
    "\n",
    "4\n",
    "00:00:08,550 --> 00:00:10,740\n",
    "为什么不能直接输入？\n",
    "\n",
    "5\n",
    "00:00:10,740 --> 00:00:16,125\n",
    "要理解这一点 请思考一下我们先从哪里获取这个文本\n",
    "\n",
    "6\n",
    "00:00:16,125 --> 00:00:20,295\n",
    "网站是文本信息的常见来源\n",
    "\n",
    "7\n",
    "00:00:20,295 --> 00:00:26,345\n",
    "这是维基百科一个网页样本的一部分 以及对应的 HTML 标记\n",
    "\n",
    "8\n",
    "00:00:26,344 --> 00:00:28,489\n",
    "HTML 标记就是我们的原始输入\n",
    "\n",
    "9\n",
    "00:00:28,489 --> 00:00:31,494\n",
    "为了进行自然语言处理\n",
    "\n",
    "10\n",
    "00:00:31,495 --> 00:00:36,245\n",
    "一般需要清除所有或者大部分 HTML 标记\n",
    "\n",
    "11\n",
    "00:00:36,244 --> 00:00:38,984\n",
    "仅保留纯文本\n",
    "\n",
    "12\n",
    "00:00:38,984 --> 00:00:44,894\n",
    "还可以清除或者提取任何 URL 或者与你的任务无关的其它项目\n",
    "\n",
    "13\n",
    "00:00:44,895 --> 00:00:50,440\n",
    "Web 可能是最常见、发展最快的文本内容来源\n",
    "\n",
    "14\n",
    "00:00:50,439 --> 00:00:53,064\n",
    "但还可能需要使用 PDF\n",
    "\n",
    "15\n",
    "00:00:53,064 --> 00:00:55,704\n",
    "Word 文件或其它文件格式\n",
    "\n",
    "16\n",
    "00:00:55,704 --> 00:00:58,000\n",
    "或者 原始输入可能来自\n",
    "\n",
    "17\n",
    "00:00:58,000 --> 00:01:02,679\n",
    "语音识别系统或者 OCR 书本扫描文件\n",
    "\n",
    "18\n",
    "00:01:02,679 --> 00:01:07,305\n",
    "了解源媒体 有助于正确处理输入\n",
    "\n",
    "19\n",
    "00:01:07,305 --> 00:01:11,650\n",
    "最终目标是提取不含任何来源标记的纯文本\n",
    "\n",
    "20\n",
    "00:01:11,650 --> 00:01:17,185\n",
    "或与任务无关的结构\n",
    "\n",
    "21\n",
    "00:01:17,185 --> 00:01:19,359\n",
    "获取纯文本之后\n",
    "\n",
    "22\n",
    "00:01:19,359 --> 00:01:22,000\n",
    "可能需要进一步处理\n",
    "\n",
    "23\n",
    "00:01:22,000 --> 00:01:26,840\n",
    "例如 大写一般不会改变词的含义\n",
    "\n",
    "24\n",
    "00:01:26,840 --> 00:01:32,030\n",
    "我们可以把所有词全部转换成大写/小写 确保以相同的方式对其进行处理\n",
    "\n",
    "25\n",
    "00:01:32,030 --> 00:01:35,799\n",
    "还可以清除用于表示停顿等的\n",
    "\n",
    "26\n",
    "00:01:35,799 --> 00:01:37,664\n",
    "标点符号\n",
    "\n",
    "27\n",
    "00:01:37,665 --> 00:01:41,245\n",
    "一个语言中的某些常用词通常能提供结构\n",
    "\n",
    "28\n",
    "00:01:41,245 --> 00:01:43,350\n",
    "但不会包含太多含义\n",
    "\n",
    "29\n",
    "00:01:43,349 --> 00:01:45,429\n",
    "例如 “一个” “和”\n",
    "\n",
    "30\n",
    "00:01:45,430 --> 00:01:48,370\n",
    "“的” “是” 等\n",
    "\n",
    "31\n",
    "00:01:48,370 --> 00:01:51,640\n",
    "有时 如果有助于\n",
    "\n",
    "32\n",
    "00:01:51,640 --> 00:01:56,000\n",
    "降低后续程序的复杂度 最好将其清除\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "00:00:00,000 --> 00:00:03,890\n",
    "好了 我们现在得到标准化纯文本\n",
    "\n",
    "2\n",
    "00:00:03,890 --> 00:00:07,700\n",
    "我们可以把它输入统计模型或机器学习模型吗？\n",
    "\n",
    "3\n",
    "00:00:07,700 --> 00:00:09,685\n",
    "不可以 我们看一下为什么\n",
    "\n",
    "4\n",
    "00:00:09,685 --> 00:00:14,125\n",
    "文本数据是利用\n",
    "\n",
    "5\n",
    "00:00:14,125 --> 00:00:19,115\n",
    "ASCII 或 Unicode 等编码 将每个字符映射到一个数字而显示在现代计算机上\n",
    "\n",
    "6\n",
    "00:00:19,114 --> 00:00:24,329\n",
    "计算机存储这些值 并以二进制形式（零和一）传输\n",
    "\n",
    "7\n",
    "00:00:24,329 --> 00:00:27,445\n",
    "这些数字还有隐含的顺序\n",
    "\n",
    "8\n",
    "00:00:27,445 --> 00:00:31,435\n",
    "65 小于 66 而 66 小于 67\n",
    "\n",
    "9\n",
    "00:00:31,434 --> 00:00:33,615\n",
    "这是否意味着 A 小于 B\n",
    "\n",
    "10\n",
    "00:00:33,615 --> 00:00:35,765\n",
    "B 小于 C？\n",
    "\n",
    "11\n",
    "00:00:35,765 --> 00:00:39,390\n",
    "不是 实际上 这是一个错误假设\n",
    "\n",
    "12\n",
    "00:00:39,390 --> 00:00:43,814\n",
    "可能会误导自然语言处理算法\n",
    "\n",
    "13\n",
    "00:00:43,814 --> 00:00:48,070\n",
    "另外 单个字符本身没有太多含义\n",
    "\n",
    "14\n",
    "00:00:48,070 --> 00:00:50,850\n",
    "我们关注的应该是词\n",
    "\n",
    "15\n",
    "00:00:50,850 --> 00:00:54,689\n",
    "但是计算机并没有针对词的标准表示法\n",
    "\n",
    "16\n",
    "00:00:54,689 --> 00:00:58,619\n",
    "是的 从内部来看 它们只是 ASCII 或 Unicode 值的序列\n",
    "\n",
    "17\n",
    "00:00:58,619 --> 00:01:04,409\n",
    "并没有捕捉词的含义或关系\n",
    "\n",
    "18\n",
    "00:01:04,409 --> 00:01:09,149\n",
    "将这种情况与计算机内存中的图像表示法相比较\n",
    "\n",
    "19\n",
    "00:01:09,150 --> 00:01:14,580\n",
    "每个像素值包含图像中这个点的相对光强\n",
    "\n",
    "20\n",
    "00:01:14,579 --> 00:01:16,045\n",
    "对于彩色图像来说\n",
    "\n",
    "21\n",
    "00:01:16,046 --> 00:01:19,215\n",
    "每个原色\n",
    "\n",
    "22\n",
    "00:01:19,215 --> 00:01:20,844\n",
    "红 绿 和蓝 都有一个值\n",
    "\n",
    "23\n",
    "00:01:20,844 --> 00:01:23,420\n",
    "这些值包含相关信息\n",
    "\n",
    "24\n",
    "00:01:23,420 --> 00:01:27,549\n",
    "值相似的两个像素在视觉上是相似的\n",
    "\n",
    "25\n",
    "00:01:27,549 --> 00:01:32,099\n",
    "因此 可以直接在值模型中使用像素值\n",
    "\n",
    "26\n",
    "00:01:32,099 --> 00:01:37,379\n",
    "可能还需要一些进行特征工程 例如边缘检测或过滤\n",
    "\n",
    "27\n",
    "00:01:37,379 --> 00:01:39,974\n",
    "但是像素是一个很好的开始\n",
    "\n",
    "28\n",
    "00:01:39,974 --> 00:01:41,594\n",
    "所以 问题在于\n",
    "\n",
    "29\n",
    "00:01:41,594 --> 00:01:44,549\n",
    "对于可以用作建模特征的文本数据\n",
    "\n",
    "30\n",
    "00:01:44,549 --> 00:01:49,019\n",
    "如何想出一个相似的表示法\n",
    "\n",
    "31\n",
    "00:01:49,019 --> 00:01:52,109\n",
    "答案取决于你使用的是哪种类型的模型\n",
    "\n",
    "32\n",
    "00:01:52,109 --> 00:01:56,099\n",
    "以及你想完成什么任务\n",
    "\n",
    "33\n",
    "00:01:56,099 --> 00:01:59,655\n",
    "如果你想使用基于图形的模型提取洞察信息\n",
    "\n",
    "34\n",
    "00:01:59,655 --> 00:02:01,760\n",
    "你可能想用包含其相互关系的\n",
    "\n",
    "35\n",
    "00:02:01,760 --> 00:02:06,660\n",
    "符号节点 如 WordNet 表示词\n",
    "\n",
    "36\n",
    "00:02:06,659 --> 00:02:09,164\n",
    "但是 对于统计模型\n",
    "\n",
    "37\n",
    "00:02:09,164 --> 00:02:12,929\n",
    "你需要使用某种值表示法\n",
    "\n",
    "38\n",
    "00:02:12,930 --> 00:02:16,349\n",
    "但你仍然需要思考最终目标是什么\n",
    "\n",
    "39\n",
    "00:02:16,349 --> 00:02:19,283\n",
    "如果你想进行文档级任务\n",
    "\n",
    "40\n",
    "00:02:19,283 --> 00:02:22,215\n",
    "例如垃圾邮件检测或情感分析\n",
    "\n",
    "41\n",
    "00:02:22,215 --> 00:02:28,229\n",
    "你可能想用文档表示法 例如词袋或 doc2vec\n",
    "\n",
    "42\n",
    "00:02:28,229 --> 00:02:31,439\n",
    "如果你想处理单个词和短语\n",
    "\n",
    "43\n",
    "00:02:31,439 --> 00:02:34,685\n",
    "例如用于文本生成或机器翻译\n",
    "\n",
    "44\n",
    "00:02:34,685 --> 00:02:39,979\n",
    "你需要用词级表示法 例如 word2vec 或 glove\n",
    "\n",
    "45\n",
    "00:02:39,979 --> 00:02:43,447\n",
    "文本信息有多种表示方法\n",
    "\n",
    "46\n",
    "00:02:43,448 --> 00:02:48,000\n",
    "只有通过实践 才能明白应该使用哪种方法解决每个问题\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "00:00:00,000 --> 00:00:04,980\n",
    "这个过程的最后一个阶段 我称之为建模\n",
    "\n",
    "2\n",
    "00:00:04,980 --> 00:00:06,850\n",
    "包括模型设计\n",
    "\n",
    "3\n",
    "00:00:06,849 --> 00:00:10,035\n",
    "一般指统计模型或机器学习模型\n",
    "\n",
    "4\n",
    "00:00:10,035 --> 00:00:15,240\n",
    "利用优化程序将其参数拟合到训练数据\n",
    "\n",
    "5\n",
    "00:00:15,240 --> 00:00:20,009\n",
    "然后用它对未知数据进行预测\n",
    "\n",
    "6\n",
    "00:00:20,010 --> 00:00:23,220\n",
    "处理值特征的优势在于\n",
    "\n",
    "7\n",
    "00:00:23,219 --> 00:00:26,984\n",
    "几乎可以使用任何机器学习模型\n",
    "\n",
    "8\n",
    "00:00:26,984 --> 00:00:30,089\n",
    "包括支持向量机 决策树\n",
    "\n",
    "9\n",
    "00:00:30,089 --> 00:00:34,229\n",
    "神经网络 或你选择的任何自定义模型\n",
    "\n",
    "10\n",
    "00:00:34,229 --> 00:00:38,069\n",
    "甚至可以组合多个模型 以获得更好的性能\n",
    "\n",
    "11\n",
    "00:00:38,070 --> 00:00:41,234\n",
    "如何使用模型则取决于你\n",
    "\n",
    "12\n",
    "00:00:41,234 --> 00:00:43,950\n",
    "你可以将它部署为基于网络的应用\n",
    "\n",
    "13\n",
    "00:00:43,950 --> 00:00:46,875\n",
    "将其包装成方便的移动应用\n",
    "\n",
    "14\n",
    "00:00:46,875 --> 00:00:49,335\n",
    "与其它产品\n",
    "\n",
    "15\n",
    "00:00:49,335 --> 00:00:51,274\n",
    "服务等集成\n",
    "\n",
    "16\n",
    "00:00:51,274 --> 00:00:53,219\n",
    "有各种可能\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
